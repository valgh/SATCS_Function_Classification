# -*- coding: utf-8 -*-
"""autoencoder_asm.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1cQd5lsOB8FLPOnvrUITYuQ8xgOSVydJS
"""

# now we can start working with our dataset.
# mounting drive...
from google.colab import drive
drive.mount("/content/gdrive/")

# now we can access the drive
# and we retrieve the data
import os
data_path = "/content/gdrive/My Drive/data/"
list_dir = os.listdir(data_path)
print(list_dir)

# we now make a dictionary out of the data directory: FUNC_NAME -> FUNC_JSON_DATA
import json
func_dict = {}
i=0
for func in list_dir:
  func_name = func[:-5]
  i+=1
  if i == 1000:
    print("reporting...\n")
    i=0
  with open(data_path+func) as f:
    func_json_data = json.load(f)
    func_dict[func_name] = func_json_data
print(func_dict['abort'])

# now we save this to a JSON file.
import json
json.dump(func_dict, open("/content/gdrive/My Drive/func_dict_raw.json", "w"))

# we can now access func_dict through the JSON file we saved
import json
func_dict = json.load(open("/content/gdrive/My Drive/func_dict_raw.json", "r"))

import networkx as nx
from networkx.readwrite import json_graph
G = json_graph.node_link_graph(func_dict['abort'])
for node in G.nodes.data():
  asm = node[1]['data']['asm_dict_string']
  for addr in asm:
    print(asm[addr])

# we now process further the previous dictionary: we want
# a dictionary so that we have: FUNC_NAME -> LIST OF ASM INSTRUCTIONS

func_dict_filtered = {}
i=0
for func_name in func_dict:
  i+=1
  if (i==1000):
    print("reporting...\n")
    i=0
  list_asm_instructions = []
  G = json_graph.node_link_graph(func_dict[func_name])
  for node in G.nodes.data():
    dict_string = node[1]['data']['asm_dict_string']
    for addr in dict_string:
      list_asm_instructions.append(dict_string[addr])
  func_dict_filtered[func_name] = list_asm_instructions
print(func_dict_filtered['abort'])

# we now create the vocabulary (wrap)
def create_vocab(func_dict_filtered):
  vocab = {}
  n = 0
  for func in func_dict_filtered:
    for inst in func_dict_filtered[func]:
      inst_f = inst.split()[0]
      if inst_f not in vocab:
        vocab[inst_f] = n
        n+=1
  return vocab

v = create_vocab(func_dict_filtered)
print(v)

idx2vocab = {}
for vocab in v:
  idx2vocab[v[vocab]] = vocab
print(idx2vocab)

# now we need to represent each instruction as a number.
# We make a wrapper function so that each possible functions is replaced properly.

def filter_instruction(instruction, vocab):
  ins = instruction.split()[0]
  return vocab.get(ins, "not existing")

print(filter_instruction("JNL", v))
print(filter_instruction("MOV", v))
print(len(v))

# now we filter all the instructions!
func_dict_final = {}
for func in func_dict_filtered:
  func_dict_final[func] = []
  list_of_asm_instructions = func_dict_filtered[func]
  for ins in list_of_asm_instructions:
    func_dict_final[func].append(filter_instruction(ins, v))

print(func_dict_final['abort'])

# now we can make a bag of words model out of our functions, like this
import numpy as np
def bow(func_dict_filtered, v):
  func_bow = {}
  for func in func_dict_filtered:
    func_bow[func] = np.zeros(len(v), dtype=int)
    list_of_asm_instructions = func_dict_filtered[func]
    for ins in list_of_asm_instructions:
      idx = filter_instruction(ins, v)
      func_bow[func][idx] = 1
  return func_bow

func_bow = bow(func_dict_filtered, v)
print(func_bow['abort'])

i=0
for func in func_bow:
  print(func_bow[func])
  i+=1
  if i == 10:
    break

# we now save the BOW model obtained
for func in func_bow:
  func_bow[func] = func_bow[func].tolist()
import json 
json.dump(func_bow, open("/content/gdrive/My Drive/func_bow.json", "w"))

# now we can definitely try our AutoEncoder
# to retrieve the encodings for each function
# but first, we need to load our dataset
# in a format that is
# made for pytorch (TensorDataset/DataLoader)
import torch
from torch.utils.data import TensorDataset, DataLoader
X_training = []
y_training = []
idx_names = []
i=0
for func in func_bow:
  y_training.append(i)
  idx_names.append(func)
  X_training.append(func_bow[func])
  i+=1

#Tensor Dataset
train_data = TensorDataset(torch.FloatTensor(X_training), torch.FloatTensor(y_training))
print(train_data)
print(train_data[1])
print(idx_names[1])
print(func_bow['fr_unset_signal'])
#DataLoader
train_loader = DataLoader(train_data, shuffle=True, batch_size=100)
print(train_loader)

# At this point we have our dataset 
# loadable from pytorch, so we can 
# train our AutoEncoder

import torch
from torch import nn
from torch import optim

# DEEP AUTOENCODER CLASS

class AutoEncoder(nn.Module):
  def __init__(self, **kwargs):
    super().__init__()
    # encoder
    self.enc1 = nn.Linear(in_features=kwargs["input_shape"], out_features=100)
    self.enc2 = nn.Linear(in_features=100, out_features=10)

    # decoder 
    self.dec1 = nn.Linear(in_features=10, out_features=100)
    self.dec2 = nn.Linear(in_features=100, out_features=kwargs["input_shape"])

  def forward(self, x):
        x = torch.relu(self.enc1(x))
        x = torch.relu(self.enc2(x))
        x = torch.relu(self.dec1(x))
        x = torch.relu(self.dec2(x))
        return x

# AUTOENCODER TRAINING STEP
# check if "cuda" GPU is available
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
# load model to device
model = AutoEncoder(input_shape=334).to(device)
print(model)
# Adam optimizer with learning rate 1e-3
optimizer = optim.Adam(model.parameters(), lr=1e-3)
# error loss function
criterion = nn.MSELoss()
epochs = 20

def train(num_epochs, train_loader, model, criterion, optimizer):
  for epoch in range(num_epochs):
    loss = 0
    for data in train_loader:
      func, label = data
      func = func.to(device)
      optimizer.zero_grad()
      output = model(func)
      train_loss = criterion(output, func)
      train_loss.backward()
      optimizer.step()
      loss += train_loss.item()
    # compute the epoch training loss
    loss = loss / len(train_loader)
    # display the epoch training loss
    print("epoch : {}/{}, recon loss = {:.8f}".format(epoch + 1, epochs, loss))
  return model

trained_model = train(epochs, train_loader, model, criterion, optimizer)

# now let's retrieve one of the decoded results

original = None
with torch.no_grad():
    for batch_features in train_loader:
        batch_features = batch_features[0]
        original = batch_features
        reconstruction = model(original)
        break

with torch.no_grad():
  n = 1
  for i in range(n):
    # original
    print(original[i])
    print(len(original[i]))
    print("\n=================\n")
    # reconstruction
    print(reconstruction[i])
    print(len(reconstruction[i]))

# we now retrieve the encodings:

print(trained_model)
original = func_bow['abort']
encoded = trained_model.enc2(trained_model.enc1(torch.FloatTensor(func_bow['abort'])))
print(original)
print(encoded.tolist())

# so now we do this:

encodings = {}
for func in func_bow:
  encoded = trained_model.enc2(trained_model.enc1(torch.FloatTensor(func_bow[func])))
  encodings[func] = encoded.tolist()

for func in encodings:
  print(func, encodings[func])
  break

print(len(encodings))

# we now save to a JSON file the encodings we obtained
json.dump(encodings, open("/content/gdrive/My Drive/asm_bow_encodings.json", "w"))
