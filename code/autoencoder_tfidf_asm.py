# -*- coding: utf-8 -*-
"""autoencoder_tfidf_asm.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/13JCsXbzzRDL_RrdF_6p7JEGDQGbaUFLe
"""

# now we can start working with our dataset.
# mounting drive...
from google.colab import drive
drive.mount("/content/gdrive/")

# we can now access func_dict through the JSON file we saved
import json
func_dict = json.load(open("/content/gdrive/My Drive/func_dict_raw_3.json", "r"))
func_dict_2 = json.load(open("/content/gdrive/My Drive/func_dict_raw_4.json", "r"))

# now we merge the dictionaries
func_dict.update(func_dict_2)
import gc
gc.collect()
print(len(func_dict))

import networkx as nx
from networkx.readwrite import json_graph
G = json_graph.node_link_graph(func_dict['abort'])
for node in G.nodes.data():
  asm = node[1]['data']['asm_dict_string']
  for addr in asm:
    print(asm[addr])

# we now process further the previous dictionary: we want
# a dictionary so that we have: FUNC_NAME -> LIST OF ASM INSTRUCTIONS
import networkx as nx
from networkx.readwrite import json_graph

func_dict_filtered = {}
i=0
for func_name in func_dict:
  i+=1
  if (i==1000):
    print("reporting...\n")
    i=0
  list_asm_instructions = []
  G = json_graph.node_link_graph(func_dict[func_name])
  for node in G.nodes.data():
    dict_string = node[1]['data']['asm_dict_string']
    for addr in dict_string:
      list_asm_instructions.append(dict_string[addr])
  func_dict_filtered[func_name] = list_asm_instructions
#print(func_dict_filtered['abort'])

# we now create the vocabulary (wrap)
def create_vocab(func_dict_filtered):
  vocab = {}
  n = 0
  for func in func_dict_filtered:
    for inst in func_dict_filtered[func]:
      inst_f = inst.split()[0]
      if inst_f not in vocab:
        vocab[inst_f] = n
        n+=1
  return vocab

v = create_vocab(func_dict_filtered)
print(v)

idx2vocab = {}
for vocab in v:
  idx2vocab[v[vocab]] = vocab
print(idx2vocab)

# now we need to represent each instruction as a number.
# We make a wrapper function so that each possible functions is replaced properly.

def filter_instruction(instruction, vocab):
  ins = instruction.split()[0]
  return vocab.get(ins, "not existing")

print(filter_instruction("JNL", v))
print(filter_instruction("MOV", v))
print(len(v))

# now we filter all the instructions!
gc.collect()
func_dict_final = {}
for func in func_dict_filtered:
  func_dict_final[func] = []
  list_of_asm_instructions = func_dict_filtered[func]
  for ins in list_of_asm_instructions:
    func_dict_final[func].append(filter_instruction(ins, v))

#print(func_dict_final['abort'])
gc.collect()

# term frequency, df and tf-idf
import math
import numpy as np
DF = {}
tot = len(v)
for func in func_dict_final:
  for token in func_dict_final[func]:
    try:
      DF[token].add(1)
    except:
      DF[token] = 1
print(len(DF))

def doc_freq(word):
    c = 0
    try:
        c = DF[word]
    except:
        pass
    return c

N = len(func_dict_final)

from collections import Counter

tf_idf = {}
for func in func_dict_final:
  tf_idf[func] = {}
  tokens = np.array(func_dict_final[func])
  counter = Counter(tokens)
  words_count = len(tokens)
  for token in np.unique(tokens):
    tf = counter[token]/words_count
    df = doc_freq(token)
    idf = np.log((N+1)/(df+1))
    tf_idf[func][token] = tf*idf

for func in tf_idf:
  for token in tf_idf[func]:
    print(func, token, tf_idf[func][token])
  break

# Function Vectorization
func_vectorized_tfidf = {}
for func in func_dict_final:
  func_vectorized_tfidf[func] = []
  for token in tf_idf[func]:
    func_vectorized_tfidf[func].append(tf_idf[func][token])

#print(func_vectorized_tfidf['abort'][0])

# some tfidf values
gc.collect()
#print(func_vectorized_tfidf['dict_attr_name_cmp'])
#print(func_vectorized_tfidf['abort'])
#print(len(tf_idf['abort']))
max_len_func = max(len(li) for li in func_vectorized_tfidf.values())
print(max_len_func)

# now we pad each function to the max value we found 
def padding(func_vectorized_tfidf):
  for func in func_vectorized_tfidf:
    act_len = len(func_vectorized_tfidf[func])
    if act_len < max_len_func:
      for i in range(max_len_func-act_len):
        func_vectorized_tfidf[func].append(0)

padding(func_vectorized_tfidf)
for func in func_vectorized_tfidf:
  if len(func_vectorized_tfidf[func]) > max_len_func:
    print(".")
  elif len(func_vectorized_tfidf[func]) < max_len_func:
    print("-")

#print(func_vectorized_tfidf['abort'])
print(len(func_vectorized_tfidf))

# we now save the model obtained
import json 
json.dump(func_vectorized_tfidf, open("/content/gdrive/My Drive/func_tfidf_77000.json", "w"))

# and we can import it
func_vectorized_tfidf = json.load(open("/content/gdrive/My Drive/func_tfidf_77000.json", "r"))

# now we can definitely try our AutoEncoder
# to retrieve the encodings for each function
# but first, we need to load our dataset
# in a format that is
# made for pytorch (TensorDataset/DataLoader)
import torch
from torch.utils.data import TensorDataset, DataLoader
X_training = []
y_training = []
idx_names = []
i=0
for func in func_vectorized_tfidf:
  y_training.append(i)
  idx_names.append(func)
  X_training.append(func_vectorized_tfidf[func])
  i+=1

#Tensor Dataset
train_data = TensorDataset(torch.FloatTensor(X_training), torch.FloatTensor(y_training))
print(train_data)
print(train_data[1])
print(idx_names[1])
#print(func_vectorized_tfidf['fr_unset_signal'])
#DataLoader
train_loader = DataLoader(train_data, shuffle=True, batch_size=100)
print(train_loader)

# At this point we have our dataset 
# loadable from pytorch, so we can 
# train our AutoEncoder

gc.collect()

import torch
from torch import nn
from torch import optim

# DEEP AUTOENCODER CLASS

class AutoEncoder(nn.Module):
  def __init__(self, **kwargs):
    super().__init__()
    # encoder
    self.enc1 = nn.Linear(in_features=kwargs["input_shape"], out_features=120)
    self.enc2 = nn.Linear(in_features=120, out_features=100)

    # decoder 
    self.dec1 = nn.Linear(in_features=100, out_features=120)
    self.dec2 = nn.Linear(in_features=120, out_features=kwargs["input_shape"])

  def forward(self, x):
        x = torch.relu(self.enc1(x))
        x = torch.relu(self.enc2(x))
        x = torch.relu(self.dec1(x))
        x = torch.relu(self.dec2(x))
        return x

# AUTOENCODER TRAINING STEP
# check if "cuda" GPU is available
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
# load model to device
model = AutoEncoder(input_shape=max_len_func).to(device)
print(model)
# Adam optimizer with learning rate 1e-3
optimizer = optim.Adam(model.parameters(), lr=1e-3)
# error loss function
criterion = nn.MSELoss()
epochs = 20

def train(num_epochs, train_loader, model, criterion, optimizer):
  for epoch in range(num_epochs):
    loss = 0
    for data in train_loader:
      func, label = data
      func = func.to(device)
      optimizer.zero_grad()
      output = model(func)
      train_loss = criterion(output, func)
      train_loss.backward()
      optimizer.step()
      loss += train_loss.item()
    # compute the epoch training loss
    loss = loss / len(train_loader)
    # display the epoch training loss
    print("epoch : {}/{}, recon loss = {:.8f}".format(epoch + 1, epochs, loss))
  return model

trained_model = train(epochs, train_loader, model, criterion, optimizer)

# we now retrieve the encoded results
gc.collect()
encodings = {}
for func in func_vectorized_tfidf:
  encoded = trained_model.enc2(trained_model.enc1(torch.FloatTensor(func_vectorized_tfidf[func])))
  encodings[func] = encoded.tolist()

for func in encodings:
  print(func, encodings[func])
  break

print(len(encodings))

# we now save to a JSON file the encodings we obtained
json.dump(encodings, open("/content/gdrive/My Drive/asm_tfidf_encodings_43000.json", "w"))