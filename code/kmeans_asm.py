# -*- coding: utf-8 -*-
"""kmeans_asm.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1h39UtUj-KshlB2PS5NKNlvT4sWdJgh9x
"""

# mounting drive...
from google.colab import drive
drive.mount("/content/gdrive/")

# retrieving the encodings
import json
# for BoW encodings (10 dimension)
#encodings = json.load(open("/content/gdrive/My Drive/asm_bow_encodings_10.json", "r"))
# for BoW encodings (20 dimension) BEST for BoW
#encodings = json.load(open("/content/gdrive/My Drive/asm_encodings.json", "r"))
# for BoW encodings (100 dimension) NOT so good
#encodings = json.load(open("/content/gdrive/My Drive/asm_bow_encodings_100.json", "r"))
# for BoW encodings with tf (10 dimension) BAD
#encodings = json.load(open("/content/gdrive/My Drive/asm_tf_bow_encodings_10.json", "r"))
# for BoW encodings with term frequency (100 dimension) VERY BAD ONE
#encodings = json.load(open("/content/gdrive/My Drive/asm_encodings_100.json", "r"))
# for tfidf encodings (50 dimension) GOOD
# encodings = json.load(open("/content/gdrive/My Drive/asm_tfidf_encodings.json", "r"))
# for tfidf encodings (100 dimension) EVEN BETTER
# encodings = json.load(open("/content/gdrive/My Drive/asm_tfidf_encodings_100.json", "r"))

encodings = json.load(open("/content/gdrive/My Drive/asm_tfidf_encodings_77000.json", "r"))
encodings_2 = json.load(open("/content/gdrive/My Drive/asm_tfidf_encodings_43000.json", "r"))
encodings.update(encodings_2)

for func in encodings:
  print(func, encodings[func])
  break
print(len(encodings))

# K-MEANS ++ algorithm class

class KMeansPlusPlus(object):

	def __init__(self, k, input_data):
		self.k = k
		self.input = input_data
		self.centroids = []
		self.clusters_utility = {}

	#squared euclidean distance between two points
	def distance(self, p1, p2):
		return np.sum(np.power(np.subtract(p1,p2), 2))

	#get the clusters' current centers:
	def get_centers(self):
		return self.centroids

	def plus_plus_initialization(self):
		#first select a random centroid
		self.centroids.append(self.input[np.random.randint(self.input.shape[0]), :])
		#now for the rest of the centroids:
		for c in range(self.k-1):
			#distances of data points from nearest centroid:
			dist = []
			for p in range(self.input.shape[0]):
				point = self.input[p,:]
			#for point in self.input:
				d = math.inf
				#compute all the distances of point from previously selected centroid
				#and select the minimum one
				for j in range(len(self.centroids)):
					tmp = self.distance(point, self.centroids[j])
					d = min(d, tmp)
				dist.append(d)

			#next centroid as far as we can go
			dist = np.array(dist)
			next_ = self.input[np.argmax(dist), :]
			self.centroids.append(next_)
			dist = []
		return self.centroids

	def check_if_equal(self, curr, new):
		#checks termination condition
		if not curr:
			return False
		else:
			return np.array_equal(np.array(curr), np.array(new))

	def get_mean(self, in_cluster, dimensionality):
		cd = []
		pts = np.array(in_cluster)
		l = pts.shape[0]
		for dim in range(dimensionality):
			cd.append(np.sum(pts[:,dim])/l)
		mean = np.array(cd)
		return mean

	def k_means(self):
		current_config = []
		clusters = {}
		while self.check_if_equal(current_config, self.centroids) == False:
			#memorize the current configuration of the clusters'centers:
			current_config.clear()
			for c in self.get_centers():
				current_config.append(c)
			clusters.clear()
			self.clusters_utility.clear()
			for c in current_config:
				clusters[tuple(c)] = []
				self.clusters_utility[tuple(c)] = []
			#for every point in the dataset
			counter = 0
			for point in self.input:
				d = math.inf
				current_centroid = tuple(current_config[0])
				for c in current_config:
					tmp_d = self.distance(point, c) #distance point-centroid
					if tmp_d<d:
						current_centroid = tuple(c)
						d = tmp_d
				clusters[current_centroid].append(point)
				self.clusters_utility[current_centroid].append(counter)
				counter+=1
			#finished clustering, now recomputing the centroids for each cluster
			self.centroids.clear()
			dim = self.input[0].shape[0]
			for c in clusters:
				new_c = self.get_mean(clusters[c], dim)
				self.centroids.append(new_c)
		return clusters

# we put k-means++ at work so that we can cluster the functions

import numpy as np
import math
from random import gauss
import time
import matplotlib.pyplot as plt
data = np.ndarray(shape=(len(encodings), 100), dtype=np.float32, order='F')

# we write each func representation to the corresponding line in data

idx = 0
idx2name = {}
for func in encodings:
  row = encodings[func]
  data[idx] = np.array(row)
  idx2name[idx] = func
  idx+=1
#print(data[0])
#for func in encodings:
#  print(encodings[func])
#  break

# we initialize the k-means++ class
# we try to retrieve k classes from our 30 000 functions

k = 10
clustering = KMeansPlusPlus(k, data)

# and now we obtain the clusters

print("========================COMPUTING...====================================")
clustering.plus_plus_initialization()
clusters = clustering.k_means()
print("\n===DONE!===\n")

# let's see what this is about
#for centroid in clusters:
#  print(centroid)
#  break

# seems to be working, now
# we need to retrieve the classes/clusters
# in clusters.clusters_utility we have a dictionary
# of CENTROID -> IDX OF FUNCTION NAME and
# in idx2name we have a dictionary IDX -> FUNCTION NAME 
# so we now obtain a new dictionary of CLUSTER -> LIST OF FUNCTION NAMES.
# We have now clustered the functions into classes.

clusters_final = {}
clusters_final_names = {}
for idx in range(k):
  clusters_final[idx] = []
  clusters_final_names[idx] = []
idx=0
for centroid in clustering.clusters_utility:
  clusters_final[idx] = clustering.clusters_utility[centroid]
  idx+=1

for centroid in clustering.clusters_utility:
  print(clustering.clusters_utility[centroid])
  break

#print(clusters_final[0])
for centroid in clusters_final:
  for idx in clusters_final[centroid]:
    clusters_final_names[centroid].append(idx2name[idx])

#print(clusters_final_names[0])

for centroid in clusters_final_names:
  print(len(clusters_final_names[centroid]))

# plot the clusters?

# we can finally save our classified functions into a JSON file:
json.dump(clusters_final_names, open("/content/gdrive/My Drive/func_classified_asm_tfidf_10_120000.json", "w"))
